# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kJEbk8hMxji2ZlSOsJgXzKGuHdeEyQjd
"""

# Import libraries
import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from transformers import TrainingArguments
from trl import SFTTrainer

# Set model and training parameters
MODEL_NAME = "mistralai/Ministral-8B-Instruct-2410"  # You can also try "meta-llama/Llama-3.1-8B"
MICRO_BATCH_SIZE = 1  # Start small to avoid OOM errors
EPOCHS = 3

# Load the dataset
dataset = load_dataset('json', data_files='/content/drive/MyDrive/minecraft-llm-finetune/training_data/minecraft_dataset.jsonl')
dataset = dataset['train'].train_test_split(test_size=0.05)

# Check dataset format
print(dataset['train'][0])

# Initialize the model with Unsloth optimizations
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,
    device_map="auto"
)

# Add LoRA adapters for efficient fine-tuning
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Lower this if you encounter memory issues
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.05
)

# Display model parameters
print(f"Trainable parameters: {model.get_nb_trainable_parameters()}")

# Setup training arguments
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/minecraft-llm-finetune/model",
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=MICRO_BATCH_SIZE,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    optim="adamw_torch",
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    warmup_ratio=0.05,
    lr_scheduler_type="cosine",
    fp16=True,
    save_total_limit=3,
    push_to_hub=False,
)

# Prepare training function
def formatting_func(example):
    # Adjust format based on your model (this is for Mistral Instruct)
    text = f"<s>[INST] {example['instruction']} [/INST] {example['output']}</s>"
    return text

# Create SFT trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    args=training_args,
    packing=True,
    formatting_func=formatting_func)

# Start training
trainer.train()

# Save the trained model
trainer.save_model("/content/drive/MyDrive/minecraft-llm-finetune/model_final")
print("Training complete! Model saved.")

# Load the fine-tuned model for testing
from unsloth import FastLanguageModel
import torch

# Load saved model
model_path = "/content/drive/MyDrive/minecraft-llm-finetune/model_final"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_path,
    max_seq_length=1024,
    dtype=torch.float16,
    load_in_4bit=True
)
FastLanguageModel.for_inference(model)

# Function to generate responses
def generate_response(prompt, max_length=1024):
    formatted_prompt = f"<s>[INST] {prompt} [/INST]"

    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_length=max_length,
        temperature=0.4,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split('[/INST]')[1].strip()

# Test with some Minecraft queries
test_prompts = [
    "mine that diamond ore",
    "craft me a diamond pickaxe",
    "come help me build a house",
    "make some torches",
    "what's the best way to find diamonds?"
]

for prompt in test_prompts:
    print(f"\nPrompt: {prompt}")
    print("-" * 40)
    response = generate_response(prompt)
    print(f"Response:\n{response}")
    print("=" * 60)